{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPpMbJqUfVHoRayjcqmJ80m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/envest99/AlternativeAssets-Scorecard/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1KipWt0hvIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import collections\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WXM7f8Mhw-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "8ef3e508-ebdb-462b-867e-4c7d1fed9f12"
      },
      "source": [
        "pip install scrapy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.6/dist-packages (2.0.1)\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (4.2.6)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.6/dist-packages (from scrapy) (2.0.5)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (2.9)\n",
            "Requirement already satisfied: Twisted>=17.9.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (20.3.0)\n",
            "Requirement already satisfied: service-identity>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (18.1.0)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.5.2)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.21.0)\n",
            "Requirement already satisfied: zope.interface>=4.1.3 in /usr/local/lib/python3.6/dist-packages (from scrapy) (5.0.2)\n",
            "Requirement already satisfied: pyOpenSSL>=16.2.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (19.1.0)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.1.0)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.6/dist-packages (from scrapy) (0.1.16)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.5.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->scrapy) (1.14.0)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->scrapy) (1.12.0)\n",
            "Requirement already satisfied: Automat>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
            "Requirement already satisfied: incremental>=16.10.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (17.5.0)\n",
            "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (2.0.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (19.3.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (19.0.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.1.3->scrapy) (46.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXZ54Yr5h6KQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scrapy\n",
        "import json\n",
        "from scrapy.crawler import CrawlerProcess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0C2bHB2iInm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class JsonWriterPipeline(object):\n",
        "\n",
        "    def open_spider(self, spider):\n",
        "        self.file = open('quoteresult.jl', 'w')\n",
        "\n",
        "    def close_spider(self, spider):\n",
        "        self.file.close()\n",
        "\n",
        "    def process_item(self, item, spider):\n",
        "        line = json.dumps(dict(item)) + \"\\n\"\n",
        "        self.file.write(line)\n",
        "        return item"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfmGeFROhxHw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "ca7de829-53ea-4676-9fc4-51d0396a5955"
      },
      "source": [
        "import logging\n",
        "\n",
        "class QuotesSpider(scrapy.Spider):\n",
        "    name = \"amazon\"\n",
        "    start_urls = ['https://www.amazon.com/s?k=paint']\n",
        "    custom_settings = {\n",
        "        'LOG_LEVEL': logging.WARNING,\n",
        "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
        "        'FEED_FORMAT':'json',                                 # Used for pipeline 2\n",
        "        'FEED_URI': 'quoteresult.json'                        # Used for pipeline 2\n",
        "    }\n",
        "\n",
        "def parse(self, response):# Find the Amazon price element\n",
        "    \t    prices = response.css('.a-price .a-offscreen::text').getall()\n",
        "\n",
        "    \t    # Initialize some counters and stats objects\n",
        "    \t    stats = dict()\n",
        "    \t    values = []\n",
        "\n",
        "    \t    for price in prices:\n",
        "        \t  value = convert_money(price)\n",
        "        \t  values.append(value)\n",
        "\n",
        "    \t    # Sort our values before calculating\n",
        "    \t    values.sort()\n",
        "\n",
        "    \t    # Calculate price statistics\n",
        "    \t    stats['average_price'] = round(sum(values) / len(values), 2)\n",
        "    \t    stats['lowest_price'] = values[0]\n",
        "    \t    stats['highest_price'] = values[-1]\n",
        "    \t    Stats['total_prices'] = len(values)\n",
        "\n",
        "    \t    print(stats)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TabError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-2f6e8bfe433c>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    def parse(self, response):\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3nCJkTLiMVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "process = CrawlerProcess({\n",
        "        'USER_AGENT': \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.75 Safari/537.36\",\n",
        "        'DOWNLOAD_TIMEOUT':100,\n",
        "        'REDIRECT_ENABLED':False,\n",
        "        'SPIDER_MIDDLEWARES' : {\n",
        "            'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware':True\n",
        "        }\n",
        "    })\n",
        "process.crawl(QuotesSpider)\n",
        "process.start() # block until finished "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}